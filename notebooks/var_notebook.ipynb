{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5f487d",
   "metadata": {},
   "source": [
    "Here we will be computing 10 day VAR for S&P500 and NASDAQ100. Output count of breaches, a plot, list of breaches, and describe which index was more volatile during pandemic outbreak and subsequent recovery period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f853bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from data.fetch_data import fetch_and_save_data\n",
    "from risk.var import historical_var, parametric_var, monte_carlo_var, calculate_daily_returns\n",
    "from risk.utils import sharpe_ratio\n",
    "from config import TICKERS, START_DATE, END_DATE, PROCESSED_DATA_DIR, CONFIDENCE_LEVEL, MONTE_CARLO_SIMULATIONS\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa98a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NASDAQ data from: /home/gusta/projects/VaR_exploration/data/processed/2025-03-30_nasdaq.csv\n",
      "Loaded S&P 500 data from: /home/gusta/projects/VaR_exploration/data/processed/2025-03-30_sp500.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_295764/272368515.py:25: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  nasdaq = pd.read_csv(latest_nasdaq_file, parse_dates=True, index_col=0)\n",
      "/tmp/ipykernel_295764/272368515.py:26: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  sp500 = pd.read_csv(latest_sp500_file, parse_dates=True, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "def find_latest_file(directory, keyword):\n",
    "    files = [\n",
    "        f for f in os.listdir(directory)\n",
    "        if keyword in f and f.endswith('.csv')\n",
    "    ]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files found for keyword '{keyword}' in {directory}\")\n",
    "\n",
    "    # Extract dates from filenames\n",
    "    files_dates = [\n",
    "        (f, datetime.strptime(f.split('_')[0], '%Y-%m-%d'))\n",
    "        for f in files\n",
    "    ]\n",
    "\n",
    "    # Find the latest file by date\n",
    "    latest_file = max(files_dates, key=lambda x: x[1])[0]\n",
    "\n",
    "    return os.path.join(directory, latest_file)\n",
    "\n",
    "# Get latest NASDAQ and S&P 500 files\n",
    "latest_nasdaq_file = find_latest_file(PROCESSED_DATA_DIR, 'nasdaq')\n",
    "latest_sp500_file = find_latest_file(PROCESSED_DATA_DIR, 'sp500')\n",
    "\n",
    "# Load data\n",
    "nasdaq = pd.read_csv(latest_nasdaq_file, parse_dates=True, index_col=0)\n",
    "sp500 = pd.read_csv(latest_sp500_file, parse_dates=True, index_col=0)\n",
    "\n",
    "print(f\"Loaded NASDAQ data from: {latest_nasdaq_file}\")\n",
    "print(f\"Loaded S&P 500 data from: {latest_sp500_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "965e4222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Closing Price'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Closing Price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Calculate daily log returns\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nasdaq[\u001b[33m'\u001b[39m\u001b[33mret_log\u001b[39m\u001b[33m'\u001b[39m] = np.log(\u001b[43mnasdaq\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mClosing Price\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m / nasdaq[\u001b[33m'\u001b[39m\u001b[33mClosing Price\u001b[39m\u001b[33m'\u001b[39m].shift(\u001b[32m1\u001b[39m))\n\u001b[32m      3\u001b[39m sp500[\u001b[33m'\u001b[39m\u001b[33mret_log\u001b[39m\u001b[33m'\u001b[39m] = np.log(sp500[\u001b[33m'\u001b[39m\u001b[33mClosing Price\u001b[39m\u001b[33m'\u001b[39m] / sp500[\u001b[33m'\u001b[39m\u001b[33mClosing Price\u001b[39m\u001b[33m'\u001b[39m].shift(\u001b[32m1\u001b[39m))\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Calculate 10 day forward log returns\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Closing Price'"
     ]
    }
   ],
   "source": [
    "# Calculate daily log returns\n",
    "nasdaq['ret_log'] = np.log(nasdaq['Closing Price'] / nasdaq['Closing Price'].shift(1))\n",
    "sp500['ret_log'] = np.log(sp500['Closing Price'] / sp500['Closing Price'].shift(1))\n",
    "\n",
    "# Calculate 10 day forward log returns\n",
    "nasdaq['ret_10d'] = np.log(nasdaq['Closing Price'].shift(-10) / nasdaq['Closing Price'])\n",
    "sp500['ret_10d'] = np.log(sp500['Closing Price'].shift(-10) / sp500['Closing Price'])\n",
    "\n",
    "# Calculate 21 day rolling standard deviation of the log returns\n",
    "nasdaq['std_21'] = nasdaq['ret_log'].rolling(window=21).std()\n",
    "sp500['std_21'] = sp500['ret_log'].rolling(window=21).std()\n",
    "\n",
    "# Calculate 99% VaR, the left tail cutoff is approx z = -2.33\n",
    "nasdaq['var_10d'] = -2.33 * 10 ** 0.5 * nasdaq['std_21']\n",
    "sp500['var_10d'] = -2.33 * 10 ** 0.5 * sp500['std_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where breaches occured by checking where return was lower than the calculacted var\n",
    "nq100['breach'] = (nq100['ret_10d'] < nq100['var_10d']) & (nq100['ret_10d'] < 0)\n",
    "sp500['breach'] = (sp500['ret_10d'] < sp500['var_10d']) & (sp500['ret_10d'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d881687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and percentage VAR breaches per index\n",
    "print('NASDAQ100 breaches count:', nq100['breach'][nq100['breach'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(nq100['breach'].mean(),3))\n",
    "print('S&P500 breaches count:', sp500['breach'][sp500['breach'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(sp500['breach'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c83a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib to do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a function (will also be used for question 5) so we can create multiple graphs\n",
    "def plot_var_breaches(data, title, breach, ax):\n",
    "    # Plotting the 10-day returns\n",
    "    ax.plot(data.index, data['ret_10d'], \n",
    "            label='10 day returns', \n",
    "            color='blue')\n",
    "    \n",
    "    # Creating a marker for breaches\n",
    "    breaches = data[data[breach]]\n",
    "    ax.scatter(breaches.index, breaches['ret_10d'],\n",
    "               color='red', \n",
    "               marker='x', \n",
    "               label='VaR Breaches')\n",
    "    \n",
    "    # Plot the 10-day VAR\n",
    "    ax.plot(data.index, data['var_10d'], \n",
    "            label='10 day VaR', \n",
    "            color='green')\n",
    "    \n",
    "    # Titles and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Returns')\n",
    "    ax.legend()\n",
    "\n",
    "# Create two subplots so we can compare the S&P500 to the NASDAQ100 VAR breaches\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Plotting the two indeces\n",
    "plot_var_breaches(nq100, 'NASDAQ100 VaR Breaches', 'breach', ax1)\n",
    "plot_var_breaches(sp500, 'S&P500 VaR Breaches','breach', ax2)\n",
    "\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ100 list of VAR breaches\n",
    "nq100_breach_table = nq100[['Closing Price', 'ret_log', 'var_10d', 'ret_10d', 'breach']]\n",
    "print('NASDAQ100', nq100_breach_table[nq100_breach_table['breach']==True].head())\n",
    "\n",
    "#S&P500 list of VAR breaches\n",
    "sp500_breach_table = sp500[['Closing Price', 'ret_log', 'var_10d', 'ret_10d', 'breach']]\n",
    "print('S&P500', sp500_breach_table[sp500_breach_table['breach']==True].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only breaches\n",
    "nq100_breach_table = nq100_breach_table[nq100_breach_table['breach']==True]\n",
    "sp500_breach_table = sp500_breach_table[sp500_breach_table['breach']==True]\n",
    "\n",
    "# Filter for covid period\n",
    "nq100_covid_breaches = nq100_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "sp500_covid_breaches = sp500_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "\n",
    "# Filter for recovery Period\n",
    "nq100_recovery_breaches = nq100_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "sp500_recovery_breaches = sp500_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "\n",
    "# Print number of breaches during each period\n",
    "print('NASDAQ100 covid breaches:', len(nq100_covid_breaches),\n",
    "      'Recovery breaches:', len(nq100_recovery_breaches))\n",
    "\n",
    "# Print number of breaches during each period\n",
    "print('S&P500 covid breaches:', len(sp500_covid_breaches), \n",
    "      'Recovery breaches:', len(sp500_recovery_breaches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baa8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick graph to inspect the covid period breaches\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Plotting the two indeces\n",
    "plot_var_breaches(nq100_covid_breaches, 'NASDAQ100 COVID VaR Breaches', 'breach', ax1)\n",
    "plot_var_breaches(sp500_covid_breaches, 'S&P500 VaR COVID Breaches','breach', ax2)\n",
    "\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4fa4b",
   "metadata": {},
   "source": [
    "The S&P500 index appears more risky than the NASDAQ100 both during the covid period of february/march 2020 and the recovery period of 2021-2022. The S&P500 saw 50% more breaches in the 2 month span of the COVID outbreak, the S&P500 saw VaR breaches earlier in february than the NASDAQ, and also had twice as many breaches in early march.\n",
    "\n",
    "Compared to the covid outbreak period of 2 months the recovery period defined as the 2 year period of 2021-2022 was much calmer, with both indices having much fewer breaches in this 2 year period than they did during 2 months of covid. But still here the NASDAQ appears less risky only having 4 breaches, 2 per year compared to the S&P500 10 breaches or 5 per year.\n",
    "\n",
    "There is not enough information to draw a conclusion or inference based on this data. But an initial hypothesis is that the NASDAQ index as a tech heavy index might not be as negativly affected by covid given the peoples move to more digital lifes during lockdowns, in general technology stocks might not have been as hurt by the pandemic as other sectors since during lockdown people consumed technology more and more. It would from that perspective make sense for a tech heavy index such as NASDAQ to be less risky in such an event than a more general economy exposed index such as S&P500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2bfe3",
   "metadata": {},
   "source": [
    "#### Now let's do the same analysis but use EWMA instead of 10 day VAR and also discuss the impact of lambda on the smoothness of EWMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632aa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variance\n",
    "nq100['EWMA'] = nq100['ret_log'].var()\n",
    "sp500['EWMA'] = sp500['ret_log'].var()\n",
    "\n",
    "# Calculate EWMA = lamda*previous day EWMA + (1-lambda) * (previous day return)^2\n",
    "for i in range(1, len(nq100)):\n",
    "        nq100.loc[nq100.index[i], 'EWMA'] = (0.72 * nq100['EWMA'].iloc[i-1] +\n",
    "                                            (1 - 0.72) * nq100['ret_log'].iloc[i] ** 2)\n",
    "for i in range(1, len(sp500)):\n",
    "        sp500.loc[sp500.index[i], 'EWMA'] = (0.72 * sp500['EWMA'].iloc[i-1] + \n",
    "                                             (1 - 0.72) * sp500['ret_log'].iloc[i] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 99% var, the left tail cutoff is approx z = -2.33\n",
    "nq100['ewma_10d'] = -2.33 * (10 * nq100['EWMA']) ** 0.5\n",
    "sp500['ewma_10d'] = -2.33 * (10 * sp500['EWMA']) ** 0.5\n",
    "\n",
    "# Check for breaches\n",
    "nq100['breach_ewma'] = (nq100['ret_10d'] < nq100['ewma_10d']) & (nq100['ret_10d'] < 0)\n",
    "sp500['breach_ewma'] = (sp500['ret_10d'] < sp500['ewma_10d']) & (sp500['ret_10d'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and percentage VAR breaches per index\n",
    "print('NASDAQ100 EWMA breaches count:', \n",
    "      nq100['breach_ewma'][nq100['breach_ewma'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(nq100['breach_ewma'].mean(),3))\n",
    "print('S&P500 EWMA breaches count:', \n",
    "      sp500['breach_ewma'][sp500['breach_ewma'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(sp500['breach_ewma'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b00b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subplots so we can compare the S&P500 to the NASDAQ100 VAR breaches\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Use our previously defined plotting function  adjusting it to use EWMA breach column\n",
    "plot_var_breaches(nq100, 'NASDAQ100 EWMA VaR Breaches', 'breach_ewma', ax1)\n",
    "plot_var_breaches(sp500, 'S&P 500 EWMA VaR Breaches','breach_ewma', ax2)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ100 list of VAR breaches\n",
    "nq100_breach_table = nq100[['Closing Price', 'ret_log', 'ewma_10d', 'ret_10d', 'breach_ewma']]\n",
    "print('NASDAQ100', nq100_breach_table[nq100_breach_table['breach_ewma']==True].head())\n",
    "\n",
    "\n",
    "#S&P500 list of VAR breaches\n",
    "sp500_breach_table = sp500[['Closing Price', 'ret_log', 'ewma_10d', 'ret_10d', 'breach_ewma']]\n",
    "print('S&P500', sp500_breach_table[sp500_breach_table['breach_ewma']==True].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only breaches\n",
    "nq100_breach_table = nq100_breach_table[nq100_breach_table['breach_ewma']==True]\n",
    "sp500_breach_table = sp500_breach_table[sp500_breach_table['breach_ewma']==True]\n",
    "\n",
    "# Filter for covid period\n",
    "nq100_covid_breaches = nq100_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "sp500_covid_breaches = sp500_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "\n",
    "# Filter for recovery Period\n",
    "nq100_recovery_breaches = nq100_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "sp500_recovery_breaches = sp500_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "\n",
    "# Print number of breaches during each period\n",
    "print('NASDAQ100 covid EWMA breaches:', \n",
    "      len(nq100_covid_breaches), \n",
    "      'Recovery EWMA breaches:', \n",
    "      len(nq100_recovery_breaches))\n",
    "print('S&P500 covid EWMA breaches:', \n",
    "      len(sp500_covid_breaches), \n",
    "      'Recovery EWMA breaches:', \n",
    "      len(sp500_recovery_breaches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a3523",
   "metadata": {},
   "source": [
    " λ impacts the smoothness of predicted EWMA volatility: The higher the λ the more weight is being given to the most recent variance, and less to the most recent return. Volatility tends to cluster together while returns follow a random walk, therefore giving more weight (higher λ) to the most recent variance results in a smoother predicted volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56addc6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_VaR",
   "language": "python",
   "name": "venv_var"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
