{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5f487d",
   "metadata": {},
   "source": [
    "Here we will be computing 10 day VAR for S&P500 and NASDAQ100. Output count of breaches, a plot, list of breaches, and describe which index was more volatile during pandemic outbreak and subsequent recovery period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f853bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'calculate_daily_returns' from 'risk.utils' (/home/gusta/projects/VaR_exploration/notebooks/../src/risk/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Import custom modules\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfetch_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_and_save_data\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrisk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m historical_var, parametric_var, monte_carlo_var, calculate_daily_returns\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrisk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sharpe_ratio\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TICKERS, START_DATE, END_DATE, PROCESSED_DATA_DIR, CONFIDENCE_LEVEL, MONTE_CARLO_SIMULATIONS\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/notebooks/../src/risk/var.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PROCESSED_DATA_DIR, CONFIDENCE_LEVEL, MONTE_CARLO_SIMULATIONS\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrisk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_daily_returns\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhistorical_var\u001b[39m(returns, confidence_level=\u001b[32m0.95\u001b[39m):\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    Calculates historical VaR given returns.\u001b[39;00m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03m        float: Historical VaR.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'calculate_daily_returns' from 'risk.utils' (/home/gusta/projects/VaR_exploration/notebooks/../src/risk/utils.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Import custom modules\n",
    "from data.fetch_data import fetch_and_save_data\n",
    "from risk.var import historical_var, parametric_var, monte_carlo_var, calculate_daily_returns\n",
    "from risk.utils import sharpe_ratio\n",
    "from config import TICKERS, START_DATE, END_DATE, PROCESSED_DATA_DIR, CONFIDENCE_LEVEL, MONTE_CARLO_SIMULATIONS\n",
    "\n",
    "# Optional styling\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa98a77",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nasdaq100_2024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load index return series, \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# setting date column to datetime format and setting as index column of dataframe\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m nq100 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnasdaq100_2024.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m sp500 =pd.read_csv(\u001b[33m'\u001b[39m\u001b[33msp500_2024.csv\u001b[39m\u001b[33m'\u001b[39m, parse_dates = [\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m], index_col = \u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/VaR_exploration/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'nasdaq100_2024.csv'"
     ]
    }
   ],
   "source": [
    "# Load index return series, \n",
    "# setting date column to datetime format and setting as index column of dataframe\n",
    "nq100 = pd.read_csv('nasdaq100_2024.csv', parse_dates = ['Date'], index_col = 'Date')\n",
    "sp500 =pd.read_csv('sp500_2024.csv', parse_dates = ['Date'], index_col = 'Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965e4222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate daily log returns\n",
    "nq100['ret_log'] = np.log(nq100['Closing Price'] / nq100['Closing Price'].shift(1))\n",
    "sp500['ret_log'] = np.log(sp500['Closing Price'] / sp500['Closing Price'].shift(1))\n",
    "\n",
    "# Calculate 10 day forward log returns\n",
    "nq100['ret_10d'] = np.log(nq100['Closing Price'].shift(-10) / nq100['Closing Price'])\n",
    "sp500['ret_10d'] = np.log(sp500['Closing Price'].shift(-10) / sp500['Closing Price'])\n",
    "\n",
    "# Calculate 21 day rolling standard deviation of the log returns\n",
    "nq100['std_21'] = nq100['ret_log'].rolling(window=21).std()\n",
    "sp500['std_21'] = sp500['ret_log'].rolling(window=21).std()\n",
    "\n",
    "# Calculate 99% VaR, the left tail cutoff is approx z = -2.33\n",
    "nq100['var_10d'] = -2.33 * 10 ** 0.5 * nq100['std_21']\n",
    "sp500['var_10d'] = -2.33 * 10 ** 0.5 * sp500['std_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where breaches occured by checking where return was lower than the calculacted var\n",
    "nq100['breach'] = (nq100['ret_10d'] < nq100['var_10d']) & (nq100['ret_10d'] < 0)\n",
    "sp500['breach'] = (sp500['ret_10d'] < sp500['var_10d']) & (sp500['ret_10d'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d881687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and percentage VAR breaches per index\n",
    "print('NASDAQ100 breaches count:', nq100['breach'][nq100['breach'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(nq100['breach'].mean(),3))\n",
    "print('S&P500 breaches count:', sp500['breach'][sp500['breach'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(sp500['breach'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c83a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib to do the plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a function (will also be used for question 5) so we can create multiple graphs\n",
    "def plot_var_breaches(data, title, breach, ax):\n",
    "    # Plotting the 10-day returns\n",
    "    ax.plot(data.index, data['ret_10d'], \n",
    "            label='10 day returns', \n",
    "            color='blue')\n",
    "    \n",
    "    # Creating a marker for breaches\n",
    "    breaches = data[data[breach]]\n",
    "    ax.scatter(breaches.index, breaches['ret_10d'],\n",
    "               color='red', \n",
    "               marker='x', \n",
    "               label='VaR Breaches')\n",
    "    \n",
    "    # Plot the 10-day VAR\n",
    "    ax.plot(data.index, data['var_10d'], \n",
    "            label='10 day VaR', \n",
    "            color='green')\n",
    "    \n",
    "    # Titles and labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Returns')\n",
    "    ax.legend()\n",
    "\n",
    "# Create two subplots so we can compare the S&P500 to the NASDAQ100 VAR breaches\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Plotting the two indeces\n",
    "plot_var_breaches(nq100, 'NASDAQ100 VaR Breaches', 'breach', ax1)\n",
    "plot_var_breaches(sp500, 'S&P500 VaR Breaches','breach', ax2)\n",
    "\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ100 list of VAR breaches\n",
    "nq100_breach_table = nq100[['Closing Price', 'ret_log', 'var_10d', 'ret_10d', 'breach']]\n",
    "print('NASDAQ100', nq100_breach_table[nq100_breach_table['breach']==True].head())\n",
    "\n",
    "#S&P500 list of VAR breaches\n",
    "sp500_breach_table = sp500[['Closing Price', 'ret_log', 'var_10d', 'ret_10d', 'breach']]\n",
    "print('S&P500', sp500_breach_table[sp500_breach_table['breach']==True].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d4013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only breaches\n",
    "nq100_breach_table = nq100_breach_table[nq100_breach_table['breach']==True]\n",
    "sp500_breach_table = sp500_breach_table[sp500_breach_table['breach']==True]\n",
    "\n",
    "# Filter for covid period\n",
    "nq100_covid_breaches = nq100_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "sp500_covid_breaches = sp500_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "\n",
    "# Filter for recovery Period\n",
    "nq100_recovery_breaches = nq100_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "sp500_recovery_breaches = sp500_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "\n",
    "# Print number of breaches during each period\n",
    "print('NASDAQ100 covid breaches:', len(nq100_covid_breaches),\n",
    "      'Recovery breaches:', len(nq100_recovery_breaches))\n",
    "\n",
    "# Print number of breaches during each period\n",
    "print('S&P500 covid breaches:', len(sp500_covid_breaches), \n",
    "      'Recovery breaches:', len(sp500_recovery_breaches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baa8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick graph to inspect the covid period breaches\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Plotting the two indeces\n",
    "plot_var_breaches(nq100_covid_breaches, 'NASDAQ100 COVID VaR Breaches', 'breach', ax1)\n",
    "plot_var_breaches(sp500_covid_breaches, 'S&P500 VaR COVID Breaches','breach', ax2)\n",
    "\n",
    "# Display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4fa4b",
   "metadata": {},
   "source": [
    "The S&P500 index appears more risky than the NASDAQ100 both during the covid period of february/march 2020 and the recovery period of 2021-2022. The S&P500 saw 50% more breaches in the 2 month span of the COVID outbreak, the S&P500 saw VaR breaches earlier in february than the NASDAQ, and also had twice as many breaches in early march.\n",
    "\n",
    "Compared to the covid outbreak period of 2 months the recovery period defined as the 2 year period of 2021-2022 was much calmer, with both indices having much fewer breaches in this 2 year period than they did during 2 months of covid. But still here the NASDAQ appears less risky only having 4 breaches, 2 per year compared to the S&P500 10 breaches or 5 per year.\n",
    "\n",
    "There is not enough information to draw a conclusion or inference based on this data. But an initial hypothesis is that the NASDAQ index as a tech heavy index might not be as negativly affected by covid given the peoples move to more digital lifes during lockdowns, in general technology stocks might not have been as hurt by the pandemic as other sectors since during lockdown people consumed technology more and more. It would from that perspective make sense for a tech heavy index such as NASDAQ to be less risky in such an event than a more general economy exposed index such as S&P500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af2bfe3",
   "metadata": {},
   "source": [
    "#### Now let's do the same analysis but use EWMA instead of 10 day VAR and also discuss the impact of lambda on the smoothness of EWMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632aa51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variance\n",
    "nq100['EWMA'] = nq100['ret_log'].var()\n",
    "sp500['EWMA'] = sp500['ret_log'].var()\n",
    "\n",
    "# Calculate EWMA = lamda*previous day EWMA + (1-lambda) * (previous day return)^2\n",
    "for i in range(1, len(nq100)):\n",
    "        nq100.loc[nq100.index[i], 'EWMA'] = (0.72 * nq100['EWMA'].iloc[i-1] +\n",
    "                                            (1 - 0.72) * nq100['ret_log'].iloc[i] ** 2)\n",
    "for i in range(1, len(sp500)):\n",
    "        sp500.loc[sp500.index[i], 'EWMA'] = (0.72 * sp500['EWMA'].iloc[i-1] + \n",
    "                                             (1 - 0.72) * sp500['ret_log'].iloc[i] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 99% var, the left tail cutoff is approx z = -2.33\n",
    "nq100['ewma_10d'] = -2.33 * (10 * nq100['EWMA']) ** 0.5\n",
    "sp500['ewma_10d'] = -2.33 * (10 * sp500['EWMA']) ** 0.5\n",
    "\n",
    "# Check for breaches\n",
    "nq100['breach_ewma'] = (nq100['ret_10d'] < nq100['ewma_10d']) & (nq100['ret_10d'] < 0)\n",
    "sp500['breach_ewma'] = (sp500['ret_10d'] < sp500['ewma_10d']) & (sp500['ret_10d'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and percentage VAR breaches per index\n",
    "print('NASDAQ100 EWMA breaches count:', \n",
    "      nq100['breach_ewma'][nq100['breach_ewma'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(nq100['breach_ewma'].mean(),3))\n",
    "print('S&P500 EWMA breaches count:', \n",
    "      sp500['breach_ewma'][sp500['breach_ewma'] == True].count(),\n",
    "      'Percentage breaches:', \n",
    "      round(sp500['breach_ewma'].mean(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b00b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two subplots so we can compare the S&P500 to the NASDAQ100 VAR breaches\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "\n",
    "# Use our previously defined plotting function  adjusting it to use EWMA breach column\n",
    "plot_var_breaches(nq100, 'NASDAQ100 EWMA VaR Breaches', 'breach_ewma', ax1)\n",
    "plot_var_breaches(sp500, 'S&P 500 EWMA VaR Breaches','breach_ewma', ax2)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ100 list of VAR breaches\n",
    "nq100_breach_table = nq100[['Closing Price', 'ret_log', 'ewma_10d', 'ret_10d', 'breach_ewma']]\n",
    "print('NASDAQ100', nq100_breach_table[nq100_breach_table['breach_ewma']==True].head())\n",
    "\n",
    "\n",
    "#S&P500 list of VAR breaches\n",
    "sp500_breach_table = sp500[['Closing Price', 'ret_log', 'ewma_10d', 'ret_10d', 'breach_ewma']]\n",
    "print('S&P500', sp500_breach_table[sp500_breach_table['breach_ewma']==True].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a357bc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only breaches\n",
    "nq100_breach_table = nq100_breach_table[nq100_breach_table['breach_ewma']==True]\n",
    "sp500_breach_table = sp500_breach_table[sp500_breach_table['breach_ewma']==True]\n",
    "\n",
    "# Filter for covid period\n",
    "nq100_covid_breaches = nq100_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "sp500_covid_breaches = sp500_breach_table.loc['2020-02-01':'2020-03-31']\n",
    "\n",
    "# Filter for recovery Period\n",
    "nq100_recovery_breaches = nq100_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "sp500_recovery_breaches = sp500_breach_table.loc['2021-01-01':'2022-12-31']\n",
    "\n",
    "# Print number of breaches during each period\n",
    "print('NASDAQ100 covid EWMA breaches:', \n",
    "      len(nq100_covid_breaches), \n",
    "      'Recovery EWMA breaches:', \n",
    "      len(nq100_recovery_breaches))\n",
    "print('S&P500 covid EWMA breaches:', \n",
    "      len(sp500_covid_breaches), \n",
    "      'Recovery EWMA breaches:', \n",
    "      len(sp500_recovery_breaches))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6a3523",
   "metadata": {},
   "source": [
    " λ impacts the smoothness of predicted EWMA volatility: The higher the λ the more weight is being given to the most recent variance, and less to the most recent return. Volatility tends to cluster together while returns follow a random walk, therefore giving more weight (higher λ) to the most recent variance results in a smoother predicted volatility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56addc6b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_VaR",
   "language": "python",
   "name": "venv_var"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
